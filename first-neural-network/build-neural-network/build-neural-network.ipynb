{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)  # Seed the random number generator\n",
    "        self.n_units = {}  # Set the number of nodes per layer\n",
    "        self.weights = {}  # Create dict to hold weights\n",
    "        self.num_layers = 0  # Set initial number of layer to one (input layer)\n",
    "        self.adjustments = {}  # Create dict to hold adjustments\n",
    "\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def __sum_squared_error(self, outputs, targets):\n",
    "        return 0.5 * np.mean((outputs - targets)**2)\n",
    "\n",
    "    def __forward_propagate(self, data):\n",
    "        # Progapagate through network and hold values for use in back-propagation\n",
    "        a_val = {}\n",
    "        a_val[1] = data\n",
    "        for layer in range(2, self.num_layers + 1):\n",
    "            z_val = np.dot(self.weights[layer - 1], a_val[layer - 1])\n",
    "            a_val[layer] = self.__sigmoid(z_val)\n",
    "        return a_val\n",
    "\n",
    "    def __back_propagate(self, outputs, target, learning_rate):\n",
    "        deltas = {}\n",
    "        # Delta of output layer\n",
    "        train_output = outputs[self.num_layers]\n",
    "        deltas[self.num_layers] = (train_output - target) * self.__sigmoid_derivative(train_output)\n",
    "\n",
    "        # Delta of hidden layer\n",
    "        for layer in reversed(\n",
    "                range(2, self.num_layers)):  # all layers except input/output\n",
    "            prev_deltas = deltas[layer + 1]\n",
    "            deltas[layer] = np.dot(\n",
    "                self.weights[layer].T,\n",
    "                prev_deltas) * self.__sigmoid_derivative(outputs[layer])\n",
    "\n",
    "        # Calculate adjustments based on deltas\n",
    "        for layer in range(1, self.num_layers):\n",
    "            self.adjustments[layer] = np.dot(deltas[layer + 1],\n",
    "                                              outputs[layer].T)\n",
    "            \n",
    "    def __gradient_descent(self, batch_size, learning_rate):\n",
    "        # Calculate partial derivative and take a step in that direction\n",
    "        for layer in range(1, self.num_layers):\n",
    "            self.weights[layer] -= learning_rate * self.adjustments[layer]\n",
    "\n",
    "\n",
    "    def add_input_layer(self, n_units):\n",
    "        self.n_units[0] = n_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "    def add_layer(self, n_units):\n",
    "        # Create weights with n_units specified + biases\n",
    "        prev_n_units = self.n_units[self.num_layers - 1]\n",
    "        self.weights[self.num_layers] = np.random.normal(\n",
    "            0.0, self.n_units[0]**-0.5, (n_units, prev_n_units))\n",
    "        # Initialize the adjustements for these weights to zero\n",
    "        self.adjustments[self.num_layers] = np.zeros((n_units, prev_n_units))\n",
    "        self.n_units[self.num_layers] = n_units\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def train(self,\n",
    "              inputs,\n",
    "              targets,\n",
    "              num_epochs,\n",
    "              learning_rate=0.1,\n",
    "              stop_accuracy=1e-5):\n",
    "        \n",
    "        error = []\n",
    "        start = time.time()\n",
    "        \n",
    "        for iteration in range(num_epochs):\n",
    "            \n",
    "            correct_so_far = 0\n",
    "            \n",
    "            for i in range(len(inputs)):\n",
    "                x = np.asarray(inputs[i]).reshape(784, 1)\n",
    "                y = np.asarray(targets[i]).reshape(10, 1)\n",
    "                \n",
    "                # Pass the training set through our nenral network\n",
    "                outputs = self.__forward_propagate(x)\n",
    "                \n",
    "                # Calculate the error\n",
    "                loss = self.__sum_squared_error(outputs[self.num_layers], y)\n",
    "                error.append(loss)\n",
    "\n",
    "                # Calculate Adjustments\n",
    "                self.__back_propagate(outputs, y, learning_rate)\n",
    "                \n",
    "                # Adjust Weights\n",
    "                self.__gradient_descent(i, learning_rate)\n",
    "                \n",
    "                if(targets[i].argmax() == outputs[self.num_layers].flatten().argmax()):\n",
    "                    correct_so_far += 1\n",
    "                \n",
    "                data_per_second = i / float(time.time() - start)\n",
    "                \n",
    "                sys.stdout.write(\"\\rProgress:\" + ('%.3f' % (100 * i / float(len(inputs))))\n",
    "                                 + \"% Speed(data/sec):\" + ('%.3f' % data_per_second)\n",
    "                                 + \" Lost:\" + ('%.3f' % loss)\n",
    "                                 + \" #Correct:\" + str(correct_so_far)\n",
    "                                 + \" #Trained:\" + str(i + 1)\n",
    "                                 + \" Training Accuracy:\" + ('%.3f' % (correct_so_far * 100 / float(i + 1))) + \"%\")\n",
    "                \n",
    "                if(i % 2500 == 0):\n",
    "                    print(\"\")\n",
    "            \n",
    "            print(\"\\nEpoch: {}, Lost: {}\".format(iteration + 1, np.mean(error[-4:])))\n",
    "\n",
    "            # Check if accuarcy criterion is satisfied\n",
    "            if np.mean(error[-(i + 1):]) < stop_accuracy and iteration > 0:\n",
    "                 break\n",
    "\n",
    "        return (np.asarray(error), iteration + 1)\n",
    "\n",
    "    def predict(self, data):\n",
    "        outputs = []\n",
    "        for i in range(len(data)):\n",
    "            x = np.asarray(data[i]).reshape(784, 1)\n",
    "            output = self.__forward_propagate(x)\n",
    "            outputs.append(output[self.num_layers])\n",
    "        return outputs\n",
    "\n",
    "    def test():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(n_units):\n",
    "    # Create instance of a neural network\n",
    "    nn = NeuralNetwork()\n",
    "\n",
    "    # Add layers (input layer is created by default)\n",
    "    nn.add_input_layer(n_units)\n",
    "    nn.add_layer(400)\n",
    "    nn.add_layer(32)\n",
    "    nn.add_layer(10)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Progress:0.000% Speed(data/sec):0.000 Lost:0.120 #Correct:1 #Trained:1 Training Accuracy:100.000%\n",
      "Progress:4.545% Speed(data/sec):431.529 Lost:0.044 #Correct:205 #Trained:2501 Training Accuracy:8.197%\n",
      "Progress:9.091% Speed(data/sec):429.860 Lost:0.045 #Correct:484 #Trained:5001 Training Accuracy:9.678%\n",
      "Progress:13.636% Speed(data/sec):435.556 Lost:0.026 #Correct:974 #Trained:7501 Training Accuracy:12.985%\n",
      "Progress:18.182% Speed(data/sec):437.843 Lost:0.045 #Correct:1757 #Trained:10001 Training Accuracy:17.568%\n",
      "Progress:22.727% Speed(data/sec):438.118 Lost:0.039 #Correct:2954 #Trained:12501 Training Accuracy:23.630%\n",
      "Progress:27.273% Speed(data/sec):438.379 Lost:0.033 #Correct:4577 #Trained:15001 Training Accuracy:30.511%\n",
      "Progress:31.818% Speed(data/sec):439.780 Lost:0.017 #Correct:6455 #Trained:17501 Training Accuracy:36.884%\n",
      "Progress:36.364% Speed(data/sec):440.185 Lost:0.039 #Correct:8448 #Trained:20001 Training Accuracy:42.238%\n",
      "Progress:40.909% Speed(data/sec):440.405 Lost:0.004 #Correct:10570 #Trained:22501 Training Accuracy:46.976%\n",
      "Progress:45.455% Speed(data/sec):439.079 Lost:0.000 #Correct:12724 #Trained:25001 Training Accuracy:50.894%\n",
      "Progress:50.000% Speed(data/sec):438.795 Lost:0.013 #Correct:14849 #Trained:27501 Training Accuracy:53.994%\n",
      "Progress:54.545% Speed(data/sec):437.927 Lost:0.000 #Correct:17074 #Trained:30001 Training Accuracy:56.911%\n",
      "Progress:59.091% Speed(data/sec):438.360 Lost:0.000 #Correct:19293 #Trained:32501 Training Accuracy:59.361%\n",
      "Progress:63.636% Speed(data/sec):438.049 Lost:0.000 #Correct:21520 #Trained:35001 Training Accuracy:61.484%\n",
      "Progress:68.182% Speed(data/sec):437.720 Lost:0.000 #Correct:23752 #Trained:37501 Training Accuracy:63.337%\n",
      "Progress:72.727% Speed(data/sec):437.887 Lost:0.002 #Correct:26011 #Trained:40001 Training Accuracy:65.026%\n",
      "Progress:77.273% Speed(data/sec):438.874 Lost:0.000 #Correct:28245 #Trained:42501 Training Accuracy:66.457%\n",
      "Progress:81.818% Speed(data/sec):437.829 Lost:0.002 #Correct:30481 #Trained:45001 Training Accuracy:67.734%\n",
      "Progress:86.364% Speed(data/sec):436.941 Lost:0.001 #Correct:32747 #Trained:47501 Training Accuracy:68.940%\n",
      "Progress:90.909% Speed(data/sec):437.437 Lost:0.000 #Correct:35014 #Trained:50001 Training Accuracy:70.027%\n",
      "Progress:95.455% Speed(data/sec):437.951 Lost:0.000 #Correct:37334 #Trained:52501 Training Accuracy:71.111%\n",
      "Progress:99.998% Speed(data/sec):438.188 Lost:0.001 #Correct:39694 #Trained:55000 Training Accuracy:72.171%\n",
      "Epoch: 1, Lost: 0.000791350169644912\n",
      "Progress:0.000% Speed(data/sec):0.000 Lost:0.003 #Correct:1 #Trained:1 Training Accuracy:100.000%\n",
      "Progress:4.545% Speed(data/sec):19.055 Lost:0.000 #Correct:2297 #Trained:2501 Training Accuracy:91.843%\n",
      "Progress:9.091% Speed(data/sec):36.491 Lost:0.000 #Correct:4582 #Trained:5001 Training Accuracy:91.622%\n",
      "Progress:13.636% Speed(data/sec):52.515 Lost:0.000 #Correct:6885 #Trained:7501 Training Accuracy:91.788%\n",
      "Progress:18.182% Speed(data/sec):67.312 Lost:0.000 #Correct:9154 #Trained:10001 Training Accuracy:91.531%\n",
      "Progress:22.727% Speed(data/sec):81.005 Lost:0.001 #Correct:11453 #Trained:12501 Training Accuracy:91.617%\n",
      "Progress:27.273% Speed(data/sec):93.527 Lost:0.000 #Correct:13780 #Trained:15001 Training Accuracy:91.861%\n",
      "Progress:31.818% Speed(data/sec):105.141 Lost:0.000 #Correct:16119 #Trained:17501 Training Accuracy:92.103%\n",
      "Progress:36.364% Speed(data/sec):116.099 Lost:0.007 #Correct:18407 #Trained:20001 Training Accuracy:92.030%\n",
      "Progress:40.909% Speed(data/sec):126.123 Lost:0.000 #Correct:20726 #Trained:22501 Training Accuracy:92.111%\n",
      "Progress:45.455% Speed(data/sec):135.699 Lost:0.000 #Correct:23059 #Trained:25001 Training Accuracy:92.232%\n",
      "Progress:50.000% Speed(data/sec):144.144 Lost:0.021 #Correct:25355 #Trained:27501 Training Accuracy:92.197%\n",
      "Progress:54.545% Speed(data/sec):152.775 Lost:0.000 #Correct:27712 #Trained:30001 Training Accuracy:92.370%\n",
      "Progress:59.091% Speed(data/sec):160.500 Lost:0.000 #Correct:30053 #Trained:32501 Training Accuracy:92.468%\n",
      "Progress:63.636% Speed(data/sec):168.102 Lost:0.000 #Correct:32400 #Trained:35001 Training Accuracy:92.569%\n",
      "Progress:68.182% Speed(data/sec):175.420 Lost:0.000 #Correct:34721 #Trained:37501 Training Accuracy:92.587%\n",
      "Progress:72.727% Speed(data/sec):182.216 Lost:0.000 #Correct:37071 #Trained:40001 Training Accuracy:92.675%\n",
      "Progress:77.273% Speed(data/sec):188.730 Lost:0.000 #Correct:39388 #Trained:42501 Training Accuracy:92.675%\n",
      "Progress:81.818% Speed(data/sec):194.934 Lost:0.001 #Correct:41720 #Trained:45001 Training Accuracy:92.709%\n",
      "Progress:86.364% Speed(data/sec):200.804 Lost:0.000 #Correct:44077 #Trained:47501 Training Accuracy:92.792%\n",
      "Progress:90.909% Speed(data/sec):206.379 Lost:0.000 #Correct:46416 #Trained:50001 Training Accuracy:92.830%\n",
      "Progress:95.455% Speed(data/sec):211.788 Lost:0.000 #Correct:48805 #Trained:52501 Training Accuracy:92.960%\n",
      "Progress:99.998% Speed(data/sec):216.827 Lost:0.000 #Correct:51209 #Trained:55000 Training Accuracy:93.107%\n",
      "Epoch: 2, Lost: 0.00020106082312644578\n",
      "Epoch: 2, Lost: 0.00020106082312644578\n",
      "Test accuracy: 0.936\n"
     ]
    }
   ],
   "source": [
    "import tflearn.datasets.mnist as mnist\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "trainX, trainY, testX, testY = mnist.load_data(one_hot=True)\n",
    "\n",
    "nn = build_nn(trainX.shape[1])\n",
    "\n",
    "error, iteration = nn.train(trainX, trainY, 2)\n",
    "print('Epoch: {}, Lost: {}'.format(iteration, np.mean(error[-4:])))\n",
    "\n",
    "preds = np.array(nn.predict(testX)).argmax(axis=1).flatten()\n",
    "actual = testY.argmax(axis=1)\n",
    "test_accuracy = np.mean(preds == actual, axis=0)\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
